# core/yt_parser/ytube_parser.py
import os
import json
from datetime import datetime, timezone, timedelta
from typing import List, Dict

from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import pickle

from core.logger import logger
from core.yt_parser.video_storage import load_json, save_json
from config import Config

config = Config()

YOUTUBE_API_KEY = config.youtube_api_key
YOUR_CLIENT_SECRET_FILE = config.youtube_secret_file
TOKEN_FILE = config.token_file
USE_OAUTH = config.use_oauth

START_DATE = datetime.fromisoformat(config.start_date).replace(tzinfo=timezone.utc)
START_DAY_BEGIN = START_DATE.replace(hour=0, minute=0, second=0, microsecond=0)
START_DAY_END = START_DATE.replace(hour=23, minute=59, second=59, microsecond=999999)

CHANNELS_JSON = config.channels_json
LAST_VIDEO_JSON = config.last_video_json
DELETED_VIDS_JSON = getattr(config, "deleted_videos_json", "deleted_videos.json")


# ---------------------- Utility functions ----------------------
def load_deleted_list() -> List[str]:
    """Load deleted videoId list."""
    if not os.path.exists(DELETED_VIDS_JSON):
        save_json(DELETED_VIDS_JSON, {"deleted": []})
        return []
    data = load_json(DELETED_VIDS_JSON)
    if not isinstance(data, dict) or "deleted" not in data:
        save_json(DELETED_VIDS_JSON, {"deleted": []})
        return []
    return data["deleted"]


def parse_yt_datetime(dt: str) -> datetime:
    """Convert YouTube `publishedAt` ISO string to timezone-aware UTC datetime."""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É
        return datetime.fromisoformat(dt.replace("Z", "+00:00"))
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ YouTube datetime string '{dt}': {e}")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º "–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ" –∑–Ω–∞—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –±—É–¥–µ—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ
        # (–¥–∞—Ç–∞ –∑–∞ 5 –¥–Ω–µ–π –¥–æ –Ω–∞—á–∞–ª–∞ —Ü–µ–ª–µ–≤–æ–≥–æ –¥–Ω—è)
        return START_DAY_BEGIN - timedelta(days=5)


# ---------------------- Main Parser Class ----------------------
class YouTubeParser:
    """YouTube daily-range parser with filtering by deleted list."""

    def __init__(self):
        os.makedirs("data", exist_ok=True)

        self.youtube = self._get_youtube_service()
        self.channels = self._load_channels()
        self.last_videos = self._load_last_videos()
        self.deleted_videos = load_deleted_list()

    # ----------- API Init -----------

    def _get_youtube_service(self):
        if USE_OAUTH:
            logger.info("Using OAuth YouTube authentication...")
            return self._get_youtube_service_oauth()
        else:
            logger.info("Using API Key YouTube authentication...")
            return build("youtube", "v3", developerKey=YOUTUBE_API_KEY)

    def _get_youtube_service_oauth(self):
        scopes = ["https://www.googleapis.com/auth/youtube.readonly"]
        creds = None

        if os.path.exists(TOKEN_FILE):
            with open(TOKEN_FILE, "rb") as token:
                creds = pickle.load(token)

        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                try:
                    creds.refresh(Request())
                except Exception:
                    creds = None

            if not creds:
                flow = InstalledAppFlow.from_client_secrets_file(
                    YOUR_CLIENT_SECRET_FILE, scopes
                )
                creds = flow.run_local_server(port=0)

            with open(TOKEN_FILE, "wb") as token:
                pickle.dump(creds, token)

        return build("youtube", "v3", credentials=creds)

    # ----------- Loaders -----------

    def _load_channels(self) -> List[Dict]:
        if not os.path.exists(CHANNELS_JSON):
            logger.error(f"Channels JSON not found: {CHANNELS_JSON}")
            return []
        with open(CHANNELS_JSON, "r", encoding="utf-8") as f:
            return json.load(f)

    def _load_last_videos(self) -> Dict[str, str]:
        if os.path.exists(LAST_VIDEO_JSON):
            with open(LAST_VIDEO_JSON, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}

    def _save_last_videos(self):
        save_json(LAST_VIDEO_JSON, self.last_videos)

    # ----------- API Requests -----------

    def _get_uploads_playlist_id(self, channel_id: str) -> str | None:
        try:
            request = self.youtube.channels().list(part="contentDetails", id=channel_id)
            response = request.execute()
            return response["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–ª–µ–π–ª–∏—Å—Ç–∞ –∑–∞–≥—Ä—É–∑–æ–∫ –¥–ª—è {channel_id}: {e}")
            return None

    def _get_channel_videos_paged(self, channel_id: str) -> List[Dict]:
        playlist_id = self._get_uploads_playlist_id(channel_id)
        if not playlist_id:
            return []

        all_raw_videos = []
        next_page_token = None

        while True:
            try:
                request = self.youtube.playlistItems().list(
                    part="snippet",
                    playlistId=playlist_id,
                    maxResults=50,
                    pageToken=next_page_token,
                )
                response = request.execute()

                # --- –õ–æ–≥–∏–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ ---
                for item in response.get("items", []):
                    pub_date = parse_yt_datetime(item["snippet"]["publishedAt"])

                    # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –î–û –Ω–∞—á–∞–ª–∞ –Ω–∞—à–µ–≥–æ –¥–Ω—è, —Ç–æ –≤—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ (–±–æ–ª–µ–µ —Å—Ç–∞—Ä—ã–µ)
                    # –≤–∏–¥–µ–æ –Ω–∞–º —Ç–æ–∂–µ –Ω–µ –Ω—É–∂–Ω—ã. –ó–∞–≤–µ—Ä—à–∞–µ–º —Ü–∏–∫–ª.
                    if pub_date < START_DAY_BEGIN:
                        logger.info(
                            f"–û—Å—Ç–∞–Ω–æ–≤–∫–∞: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –≤–∏–¥–µ–æ –æ—Ç {pub_date.date()}, –±–æ–ª–µ–µ —Å—Ç–∞—Ä–æ–µ, —á–µ–º {START_DAY_BEGIN.date()}"
                        )
                        return all_raw_videos

                    # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ü–µ–ª–µ–≤–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (–∏–ª–∏ –ø–æ–∑–∂–µ, —á—Ç–æ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ –¥–ª—è –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞)
                    all_raw_videos.append(item)

                next_page_token = response.get("nextPageToken")
                if not next_page_token:
                    break

            except Exception as e:
                logger.error(
                    f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ –∏–∑ –ø–ª–µ–π–ª–∏—Å—Ç–∞ {playlist_id} –∫–∞–Ω–∞–ª–∞ {channel_id}: {e}"
                )
                break

        return all_raw_videos

    # ----------- Main Logic -----------

    def check_for_new_videos(self) -> List[Dict]:
        """
        Returns ONLY videos published on START_DATE between 00:00‚Äì23:59:59 UTC,
        excluding deleted ones and previously processed ones.
        """
        logger.info(
            f"üîç Checking for videos published on {START_DATE.date()} "
            f"from {START_DAY_BEGIN} to {START_DAY_END}"
        )

        new_videos = []

        for channel in self.channels:
            channel_id = channel["id"]
            channel_name = channel["name"]
            logger.info(f"Checking channel: {channel_id} ({channel_name})")

            videos = self._get_channel_videos_paged(channel_id)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º (–µ—Å–ª–∏ API –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ—Ä—è–¥–æ–∫)
            videos.sort(key=lambda x: x["snippet"]["publishedAt"], reverse=True)
            found_videos_for_channel = []

            for video in videos:
                vid = video["snippet"]["resourceId"]["videoId"]

                # Skip deleted
                if vid in self.deleted_videos:
                    continue

                # Skip already processed
                if vid == self.last_videos.get(channel_id):
                    continue

                snippet = video["snippet"]
                pub = parse_yt_datetime(snippet["publishedAt"])

                if not (START_DAY_BEGIN <= pub <= START_DAY_END):
                    continue

                video_data = {
                    "channel_id": channel_id,
                    "channel_name": channel_name,
                    "video_id": vid,
                    "title": snippet["title"],
                    "description": snippet.get("description", ""),
                    "thumbnail": snippet["thumbnails"]["high"]["url"],
                    "published_at": snippet["publishedAt"],
                    "url": f"https://www.youtube.com/watch?v={vid}",
                }

                new_videos.append(video_data)
                found_videos_for_channel.append(video_data)

            if found_videos_for_channel:
                # –¢–∞–∫ –∫–∞–∫ —Å–ø–∏—Å–æ–∫ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ –¥–∞—Ç–µ (newest-first), —ç–ª–µ–º–µ–Ω—Ç [0] ‚Äî —ç—Ç–æ —Å–∞–º—ã–π –Ω–æ–≤—ã–π
                self.last_videos[channel_id] = found_videos_for_channel[0]["video_id"]

        self._save_last_videos()
        logger.info(f"‚úÖ Found {len(new_videos)} videos matching date filter.")

        return new_videos
